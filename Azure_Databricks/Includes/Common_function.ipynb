{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80344bd9-1d9e-4c63-af7c-000c48fe9185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc15eebe-826f-44f7-8635-1bc9e96ac3ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_column_value(\n",
    "    df,\n",
    "    column_name,\n",
    "    old_value1,\n",
    "    new_value1,\n",
    "    old_value2,\n",
    "    new_value2,\n",
    "    default_value=\"n/a\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Standardizes categorical column values.\n",
    "\n",
    "    - Trims whitespace\n",
    "    - Converts values to uppercase for comparison\n",
    "    - Maps known values to standardized labels\n",
    "    - Assigns default value for unknowns\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_col = F.upper(F.trim(F.col(column_name)))\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\n",
    "            column_name,\n",
    "            F.when(normalized_col == old_value1, new_value1)\n",
    "             .when(normalized_col == old_value2, new_value2)\n",
    "             .otherwise(default_value)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0ac07a-cf97-4fde-b474-2b6185e841c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def configure_adls_access(storage_account=\"salesdwh\"):\n",
    "    \"\"\"\n",
    "    Configures Spark to access Azure Data Lake Gen2 using OAuth.\n",
    "    Secrets are fetched securely from Databricks Secret Scope.\n",
    "    \"\"\"\n",
    "\n",
    "    scope_name = \"salesdwhscope\"\n",
    "\n",
    "    client_id = dbutils.secrets.get(scope=scope_name, key=\"clientid\")\n",
    "    tenant_id = dbutils.secrets.get(scope=scope_name, key=\"tenantid\")\n",
    "    client_secret = dbutils.secrets.get(scope=scope_name, key=\"clientsecret\")\n",
    "\n",
    "    account_url = f\"{storage_account}.dfs.core.windows.net\"\n",
    "\n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{account_url}\", \"OAuth\")\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.oauth.provider.type.{account_url}\",\n",
    "        \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
    "    )\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{account_url}\", client_id)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{account_url}\", client_secret)\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.oauth2.client.endpoint.{account_url}\",\n",
    "        f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c9382a-0409-46fc-8afa-0b41ac28820a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "def incremental_load(df, catalog_name, schema_name, table_name, merge_condition):\n",
    "    \"\"\"\n",
    "    Performs an upsert (MERGE) into a Delta table.\n",
    "\n",
    "    - MERGE if table exists\n",
    "    - CREATE table if not present\n",
    "    \"\"\"\n",
    "\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "    if spark.catalog.tableExists(full_table_name):\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, full_table_name)\n",
    "\n",
    "        delta_table.alias(\"tgt\") \\\n",
    "            .merge(df.alias(\"src\"), merge_condition) \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "\n",
    "        return f\"Incremental load completed for table: {full_table_name}\"\n",
    "\n",
    "    else:\n",
    "        df.write.format(\"delta\").saveAsTable(full_table_name)\n",
    "        return f\"Table created successfully: {full_table_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1e4468-64ba-4d76-830f-537134be31cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_date_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Cleans and converts an integer/string date column to DateType.\n",
    "\n",
    "    Rules:\n",
    "    - Invalid values (0 or incorrect length) → NULL\n",
    "    - Valid values → parsed using yyyyMMdd format\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\n",
    "            column_name,\n",
    "            F.when(\n",
    "                (F.col(column_name) == 0)\n",
    "                | (F.length(F.col(column_name).cast(\"string\")) != 8),\n",
    "                None\n",
    "            ).otherwise(\n",
    "                F.to_date(F.col(column_name).cast(\"string\"), \"yyyyMMdd\")\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Common_function",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
